{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5265f875-434f-47d9-a262-6912a6707d0b",
   "metadata": {},
   "source": [
    "# Stable Diffusion Model Conversion for Mobile Deployment\n",
    "\n",
    "This notebook documents the process of converting the UNet, VAE, and Text Encoder components of Stable Diffusion v1.4 to the ONNX & Pytorch Mobile format for Mobile deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079df5e9-164f-432e-b805-b614b1a9db2c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [1. Setup and Configuration](#chapter1)\n",
    "    * [1.1 Download model from HuggingFace](#section_1_1)\n",
    "    * [1.2 Set dummy inputs for inferencing](#section_1_2)\n",
    "* [2 Conversion for Mobile Deployment](#chapter2)\n",
    "    * [2.1 Text Encoder Conversion](#section_2_1)\n",
    "    * [2.2 UNet Conversion](#section_2_2)\n",
    "    * [2.3 VAE Conversion](#section_2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc02cbc-7094-4809-8acc-fd03abbe8817",
   "metadata": {},
   "source": [
    "### 1. Setup and Configuration <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e156e65-eb01-4339-9474-13415321f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Automatically install required Python packages if they are missing\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Helper function to install packages if not already installed\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Core packages\n",
    "install_if_missing(\"torch\")\n",
    "install_if_missing(\"numpy\")\n",
    "install_if_missing(\"onnxruntime\")\n",
    "install_if_missing(\"diffusers\")\n",
    "install_if_missing(\"transformers\")\n",
    "install_if_missing(\"PIL\")  \n",
    "install_if_missing(\"matplotlib\")\n",
    "install_if_missing(\"accelerate\")\n",
    "install_if_missing(\"tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6269086-b491-49be-b392-c3cf7e2f3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from diffusers import DiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import UNet2DConditionModel, PNDMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f22f3365-3c12-4c0d-9a80-b8144f1ef069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cpu\n",
      "ONNX Runtime version: 1.20.1\n",
      "Diffusers version: 0.32.2\n",
      "Transformers version: 4.48.0\n"
     ]
    }
   ],
   "source": [
    "# List packages version used in this notebook\n",
    "from diffusers import __version__ as diffusers_version\n",
    "from transformers import __version__ as transformers_version\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(f\"Diffusers version: { diffusers_version}\") \n",
    "print(f\"Transformers version: {transformers_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "313f49c4-2a16-45fa-b128-5bec35f5c6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to CPU\n"
     ]
    }
   ],
   "source": [
    "# Setting CPU or GPU (if avil) to speed up inferencing/conversion\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Device set to {device.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc372c91-a065-4729-9fd6-6cbbda2e0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting folder paths to store model files\n",
    "\n",
    "MODEL_PATH = './stable-diffusion-v1-4'\n",
    "\n",
    "UNET_FILE_PATH = 'unet/unet_onnx.onnx'\n",
    "VAE_FILE_PATH = 'vae_onnx.onnx'\n",
    "ENCODER_FILE_PATH = 'encoder_pt.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f4b63-83bc-4874-abec-4cd8f1dfdbf8",
   "metadata": {},
   "source": [
    "##### 1.1 Download model from HuggingFace <a id=\"section_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16d516f0-aa94-4ab1-877c-8f7e8e59751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from hugging face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|████████████████████████████████████████████████████| 7/7 [00:00<00:00, 16.80it/s]\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Diffusion model already exists. Skipping download\")\n",
    "else:\n",
    "    print(\"Downloading model from hugging face\")\n",
    "    pipeline = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "    pipeline.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd56915e-9760-40c4-9ef9-6be51301b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperately load each components for conversion\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(MODEL_PATH, subfolder=\"unet\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(MODEL_PATH, subfolder=\"vae\").to(device)\n",
    "text_encoder = CLIPTextModel.from_pretrained(MODEL_PATH, subfolder=\"text_encoder\").to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(MODEL_PATH, subfolder=\"tokenizer\")\n",
    "scheduler = PNDMScheduler.from_pretrained(MODEL_PATH, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7817f9d-08b1-4d59-8c24-66a0b7319274",
   "metadata": {},
   "source": [
    "#### 1.2 Set dummy inputs for inferencing <a id=\"section_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cecbb1c7-4ebe-498e-8d5e-12a37bce8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"A realistic portrait of an old man\"]\n",
    "height, width = 512, 512  # default height and width\n",
    "num_channel = 4\n",
    "num_inference_steps = 10  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.Generator(device=device).manual_seed(0)  # Seed generator to create\n",
    "batch_size = len(prompt)\n",
    "\n",
    "# Create tokens for prompt and negative prompt\n",
    "prompt_tokens = tokenizer(\n",
    "    prompt,\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "neg_tokens = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Create text embeddings for prompt and negative prompt\n",
    "prompt_embeddings = text_encoder(prompt_tokens.input_ids.to(device))[0]\n",
    "neg_embeddings =  text_encoder(neg_tokens.input_ids.to(device))[0]\n",
    "embeddings = torch.cat( [neg_embeddings, prompt_embeddings])\n",
    "\n",
    "# Generate random latent noise\n",
    "latent_noise = torch.randn((batch_size, num_channel, height // 8, width // 8)) \n",
    "\n",
    "# Generate time steps\n",
    "scheduler.set_timesteps(num_inference_steps) # Generate time step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e159292-223c-4271-b91e-8e70afd0650c",
   "metadata": {},
   "source": [
    "### 2. Conversion for Mobile Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a203ac1-4310-4c4c-94b3-ebbcafa2dd00",
   "metadata": {},
   "source": [
    "#### 2.1 Text Encoder Conversion <a id=\"section_2_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4147728-667d-416a-aa4d-6dfe18f05608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:88: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_shape[-1] > 1 or self.sliding_window is not None:\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:164: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n"
     ]
    }
   ],
   "source": [
    "class TextEncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, text_encoder):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # Get the dictionary output and return the required tensor\n",
    "        outputs = self.text_encoder(input_ids)\n",
    "        return outputs[\"last_hidden_state\"]  \n",
    "\n",
    "# Wrap the original text_encoder model\n",
    "wrapped_text_encoder = TextEncoderWrapper(text_encoder)\n",
    "\n",
    "# trace the wrapped model\n",
    "traced_model = torch.jit.trace(wrapped_text_encoder, prompt_tokens.input_ids)\n",
    "torch.jit.save(traced_model, ENCODER_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30154928-a807-49d9-b93f-eee96677eb93",
   "metadata": {},
   "source": [
    "#### 2.2 UNet Conversion <a id=\"section_2_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f79917c-a39b-430b-9ee5-42cd3b1c12d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1111: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if dim % default_overall_up_factor != 0:\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\diffusers\\models\\downsampling.py:136: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\diffusers\\models\\downsampling.py:145: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\diffusers\\models\\upsampling.py:147: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\diffusers\\models\\upsampling.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.shape[0] >= 64:\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\diffusers\\models\\upsampling.py:173: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.numel() * scale_factor > pow(2, 31):\n",
      "C:\\Users\\Desk\\Desktop\\diffusion\\.venv\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1309: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not return_dict:\n"
     ]
    }
   ],
   "source": [
    "t0 = scheduler.timesteps[0] # Only one timestep needed for UNet Conversion\n",
    "\n",
    "# expand the latents to avoid doing two forward passes.\n",
    "expand_latent_noise = torch.cat([latent_noise] * 2)\n",
    "# by design when the model is >= 2gb, ONNX export produces hundreds of weight/bias/Matmul/etc. files alongside the .onnx file\n",
    "# https://github.com/pytorch/pytorch/issues/94280\n",
    "# text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "os.makedirs(\"unet\")\n",
    "torch.onnx.export(unet, (expand_latent_noise, t0, embeddings), UNET_FILE_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d8954-0118-4848-9baf-130087c0aebf",
   "metadata": {},
   "source": [
    "#### 2.3 VAE Conversion <a id=\"section_2_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8525fce7-8b74-4355-a97c-388efbd29103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 11/11 [00:31<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latent_noise] * 2)\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "    \n",
    "    # predict the noise residual\n",
    "    with torch.inference_mode():\n",
    "        noise_pred = unet(latent_model_input, t, embeddings).sample\n",
    "    \n",
    "    # perform guidance\n",
    "    noise_pred_neg, noise_pred_prompt = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_neg + guidance_scale * (noise_pred_prompt - noise_pred_neg)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latent_noise = scheduler.step(noise_pred, t, latent_noise).prev_sample\n",
    "\n",
    "# Scale latent after denoise loop\n",
    "latent_noise = 1 / 0.18215 * latent_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76163e20-512a-45e9-ac8d-561a0b92e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEWrapper(torch.nn.Module):\n",
    "  def __init__(self, vae):\n",
    "    super(VAEWrapper, self).__init__()\n",
    "    self.vae = vae\n",
    "\n",
    "  def forward(self, latents):\n",
    "    return self.vae.decode(latents).sample\n",
    "\n",
    "\n",
    "vae_wrapper = VAEWrapper(vae)\n",
    "torch.onnx.export(vae_wrapper, latent_noise, VAE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19bd422b-d41f-463d-bc9c-bc9ef52242e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
